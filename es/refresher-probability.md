**1. Probabilities and Statistics refresher**

&#10230; Repaso de Probabilidades y Estadísticas

<br>

**2. Introduction to Probability and Combinatorics**

&#10230; Introducción a las Probabilidades y Combinatoria

<br>

**3. Sample space ― The set of all possible outcomes of an experiment is known as the sample space of the experiment and is denoted by S.**

&#10230; Espacio Muestral - El conjunto de todos los posibles resultados de un experimento es conocido como el espacio muestral del experimento y se denota como S.

<br>

**4. Event ― Any subset E of the sample space is known as an event. That is, an event is a set consisting of possible outcomes of the experiment. If the outcome of the experiment is contained in E, then we say that E has occurred.**

&#10230; Evento - Cualquier subconjunto de el espacio muestral es conocido como un evento. Esto significa que un evento es un conjunto de posibles resultados de un experimento. Si el resultado de un experimento esta contenido en E, entonces decimos que el evento E ha ocurrido.

<br>

**5. Axioms of probability - For each event E, we denote P(E) as the probability of event E occuring.**

&#10230; Axiomas de la probabilidad Para cada evento E, P(E) denota la probabilidad de que el evento E ocurra.

<br>

**6. Axiom 1 ― Every probability is between 0 and 1 included, i.e:**

&#10230; Axioma 1 - Cada probabilidad tiene valores entre 0 y 1 inclusive, esto es:

<br>

**7. Axiom 2 ― The probability that at least one of the elementary events in the entire sample space will occur is 1, i.e:**

&#10230; Axioma 2 - La probabilidad de que por lo menos uno de los eventos elementales de todo el espacio muestral ocurra es 1, esto es:

<br>

**8. Axiom 3 ― For any sequence of mutually exclusive events E1,...,En, we have:**

&#10230; Axioma 3 - Por cada secuencia de eventos mutuamente excluyentes E1,...,En, se tiene:

<br>

**9. Permutation ― A permutation is an arrangement of r objects from a pool of n objects, in a given order. The number of such arrangements is given by P(n,r), defined as:**

&#10230; Permutación - Una permutación es un arreglo de r objetos tomados de un grupo de n objetos, en un order arbitrario. El número de estos arreglos es dado por P(n,r), definido como:

<br>

**10. Combination ― A combination is an arrangement of r objects from a pool of n objects, where the order does not matter. The number of such arrangements is given by C(n,r), defined as:**

&#10230; Combinación - Una combinación es un arreglo de r objetos tomados de un grupo de n objetos, donde el orden no importa. The número de estos arreglos es dado por C(n,r), definido como:

<br>

**11. Remark: we note that for 0⩽r⩽n, we have P(n,r)⩾C(n,r)**

&#10230; Observación: cabe resaltar que para 0⩽r⩽n, se tiene P(n,r)⩾C(n,r)

<br>

**12. Conditional Probability**

&#10230; Probabilidad Condicional

<br>

**13. Bayes' rule ― For events A and B such that P(B)>0, we have:**

&#10230; Regla de Bayes - Para eventos A y B tal que P(B)>0, se tiene:

<br>

**14. Remark: we have P(A∩B)=P(A)P(B|A)=P(A|B)P(B)**

&#10230; Observación: Se tiene P(A∩B)=P(A)P(B|A)=P(A|B)P(B)

<br>

**15. Partition ― Let {Ai,i∈[[1,n]]} be such that for all i, Ai≠∅. We say that {Ai} is a partition if we have:**

&#10230; Partición - Sea {Ai,i∈[[1,n]]} tal que para todo i, Ai≠∅. Se dice entonces que {Ai} es una partición si se cumple:

<br>

**16. Remark: for any event B in the sample space, we have P(B)=n∑i=1P(B|Ai)P(Ai).**

&#10230; Observación: Para cualquier evento B del espacio muestral, se cumple P(B)=n∑i=1P(B|Ai)P(Ai).

<br>

**17. Extended form of Bayes' rule ― Let {Ai,i∈[[1,n]]} be a partition of the sample space. We have:**

&#10230; Regla de Bayes extendida - Sea {Ai,i∈[[1,n]]} una partición del espacio muestral. Se cumple:

<br>

**18. Independence ― Two events A and B are independent if and only if we have:**

&#10230; Independencia - Dos events A y B son independientes si y solo si se cumple:

<br>

**19. Random Variables**

&#10230; Variables Aleatorias

<br>

**20. Definitions**

&#10230; Definiciones

<br>

**21. Random variable ― A random variable, often noted X, is a function that maps every element in a sample space to a real line.**

&#10230; Variable Aleatoria - Una variable aleatoria, generalmente denotada por X, es una función que asocia cada elemento de un espacio muestral a una linea real.

<br>

**22. Cumulative distribution function (CDF) ― The cumulative distribution function F, which is monotonically non-decreasing and is such that limx→−∞F(x)=0 and limx→+∞F(x)=1, is defined as:**

&#10230; Función de distribución acumulada (FDA) - La función de distribución acumulada F, la cual es monótonamente creciente y es tal que limx→−∞F(x)=0 y limx→+∞F(x)=1, es definida como:

<br>

**23. Remark: we have P(a<X⩽B)=F(b)−F(a).**

&#10230; Observación: Se tiene P(a<X⩽B)=F(b)−F(a).

<br>

**24. Probability density function (PDF) ― The probability density function f is the probability that X takes on values between two adjacent realizations of the random variable.**

&#10230; Función de densidad de Probabilidad (FDP) - La función de densidad de probabilidad f es la probabilidad que X tome valores entre dos ocurrencias adyacentes de la variable aleatoria.

<br>

**25. Relationships involving the PDF and CDF ― Here are the important properties to know in the discrete (D) and the continuous (C) cases.**

&#10230; Relaciones entre la FDA y FDP - Estas son las propiedades mas importantes para conocer en los casos discreto (D) y contínuo (C).

<br>

**26. [Case, CDF F, PDF f, Properties of PDF]**

&#10230; [Caso, FDA F, FDP f, Propiedades de FDP]

<br>

**27. Expectation and Moments of the Distribution ― Here are the expressions of the expected value E[X], generalized expected value E[g(X)], kth moment E[Xk] and characteristic function ψ(ω) for the discrete and continuous cases:**

&#10230;

<br>

**28. Variance ― The variance of a random variable, often noted Var(X) or σ2, is a measure of the spread of its distribution function. It is determined as follows:**

&#10230;

<br>

**29. Standard deviation ― The standard deviation of a random variable, often noted σ, is a measure of the spread of its distribution function which is compatible with the units of the actual random variable. It is determined as follows:**

&#10230;

<br>

**30. Transformation of random variables ― Let the variables X and Y be linked by some function. By noting fX and fY the distribution function of X and Y respectively, we have:**

&#10230;

<br>

**31. Leibniz integral rule ― Let g be a function of x and potentially c, and a,b boundaries that may depend on c. We have:**

&#10230;

<br>

**32. Probability Distributions**

&#10230;

<br>

**33. Chebyshev's inequality ― Let X be a random variable with expected value μ. For k,σ>0, we have the following inequality:**

&#10230;

<br>

**34. Main distributions ― Here are the main distributions to have in mind:**

&#10230;

<br>

**35. [Type, Distribution]**

&#10230;

<br>

**36. Jointly Distributed Random Variables**

&#10230;

<br>

**37. Marginal density and cumulative distribution ― From the joint density probability function fXY , we have**

&#10230;

<br>

**38. [Case, Marginal density, Cumulative function]**

&#10230;

<br>

**39. Conditional density ― The conditional density of X with respect to Y, often noted fX|Y, is defined as follows:**

&#10230;

<br>

**40. Independence ― Two random variables X and Y are said to be independent if we have:**

&#10230;

<br>

**41. Covariance ― We define the covariance of two random variables X and Y, that we note σ2XY or more commonly Cov(X,Y), as follows:**

&#10230;

<br>

**42. Correlation ― By noting σX,σY the standard deviations of X and Y, we define the correlation between the random variables X and Y, noted ρXY, as follows:**

&#10230;

<br>

**43. Remark 1: we note that for any random variables X,Y, we have ρXY∈[−1,1].**

&#10230;

<br>

**44. Remark 2: If X and Y are independent, then ρXY=0.**

&#10230;

<br>

**45. Parameter estimation**

&#10230;

<br>

**46. Definitions**

&#10230;

<br>

**47. Random sample ― A random sample is a collection of n random variables X1,...,Xn that are independent and identically distributed with X.**

&#10230;

<br>

**48. Estimator ― An estimator is a function of the data that is used to infer the value of an unknown parameter in a statistical model.**

&#10230;

<br>

**49. Bias ― The bias of an estimator ^θ is defined as being the difference between the expected value of the distribution of ^θ and the true value, i.e.:**

&#10230;

<br>

**50. Remark: an estimator is said to be unbiased when we have E[^θ]=θ.**

&#10230;

<br>

**51. Estimating the mean**

&#10230;

<br>

**52. Sample mean ― The sample mean of a random sample is used to estimate the true mean μ of a distribution, is often noted ¯¯¯¯¯X and is defined as follows:**

&#10230;

<br>

**53. Remark: the sample mean is unbiased, i.e E[¯¯¯¯¯X]=μ.**

&#10230;

<br>

**54. Central Limit Theorem ― Let us have a random sample X1,...,Xn following a given distribution with mean μ and variance σ2, then we have:**

&#10230;

<br>

**55. Estimating the variance**

&#10230;

<br>

**56. Sample variance ― The sample variance of a random sample is used to estimate the true variance σ2 of a distribution, is often noted s2 or ^σ2 and is defined as follows:**

&#10230;

<br>

**57. Remark: the sample variance is unbiased, i.e E[s2]=σ2.**

&#10230;

<br>

**58. Chi-Squared relation with sample variance ― Let s2 be the sample variance of a random sample. We have:**

&#10230;

<br>
